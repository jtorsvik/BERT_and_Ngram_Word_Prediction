{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Natural Language Processing\n",
    "\n",
    "## Baseline model - N-Gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWYStYksGR4x",
    "outputId": "55f6ab56-5112-4d00-aacc-704580d6ca52"
   },
   "outputs": [],
   "source": [
    "# Uncomment this line when using Google Colab\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAGHnGO4GqCU",
    "outputId": "54bcce36-ae0a-4ee6-d0ab-386ee31fa362"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Joakim\n",
      "[nltk_data]     Torsvik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General useful tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random, math, csv, collections, nltk, operator\n",
    "\n",
    "#Visualization tools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP tools\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the training-, and testing data\n",
    "\n",
    "The language model will be trained on the Project Gutenberg corpus, with the Sherlock Holmes novels, written by Sir Arthur Conan Doyle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "R-BBK29-Ht2p"
   },
   "outputs": [],
   "source": [
    "# Google Colab directory path\n",
    "\n",
    "#parentdir = \"/content/drive/MyDrive/sentence-completion\"\n",
    "#trainingdir = parentdir + \"/Holmes_Training_Data\"\n",
    "#questions = os.path.join(parentdir,\"testing_data.csv\")\n",
    "#answers = os.path.join(parentdir,\"test_answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook directory path\n",
    "\n",
    "parentdir = \"C:\\\\Users\\Joakim Torsvik\\Downloads\\MSc Data Science\\Advanced NLP\\Week 2 - N-grams\\sentence-completion\"\n",
    "trainingdir = parentdir + \"/Holmes_Training_Data\"\n",
    "questions = os.path.join(parentdir, \"testing_data.csv\")\n",
    "answers = os.path.join(parentdir, \"test_answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>a)</th>\n",
       "      <th>b)</th>\n",
       "      <th>c)</th>\n",
       "      <th>d)</th>\n",
       "      <th>e)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I have it from the same source that you are bo...</td>\n",
       "      <td>crying</td>\n",
       "      <td>instantaneously</td>\n",
       "      <td>residing</td>\n",
       "      <td>matched</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>It was furnished partly as a sitting and partl...</td>\n",
       "      <td>daintily</td>\n",
       "      <td>privately</td>\n",
       "      <td>inadvertently</td>\n",
       "      <td>miserably</td>\n",
       "      <td>comfortably</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>As I descended , my old ally , the _____ , cam...</td>\n",
       "      <td>gods</td>\n",
       "      <td>moon</td>\n",
       "      <td>panther</td>\n",
       "      <td>guard</td>\n",
       "      <td>country-dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>We got off , _____ our fare , and the trap rat...</td>\n",
       "      <td>rubbing</td>\n",
       "      <td>doubling</td>\n",
       "      <td>paid</td>\n",
       "      <td>naming</td>\n",
       "      <td>carrying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>He held in his hand a _____ of blue paper , sc...</td>\n",
       "      <td>supply</td>\n",
       "      <td>parcel</td>\n",
       "      <td>sign</td>\n",
       "      <td>sheet</td>\n",
       "      <td>chorus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                           question        a)  \\\n",
       "0  1  I have it from the same source that you are bo...    crying   \n",
       "1  2  It was furnished partly as a sitting and partl...  daintily   \n",
       "2  3  As I descended , my old ally , the _____ , cam...      gods   \n",
       "3  4  We got off , _____ our fare , and the trap rat...   rubbing   \n",
       "4  5  He held in his hand a _____ of blue paper , sc...    supply   \n",
       "\n",
       "                b)             c)         d)             e)  \n",
       "0  instantaneously       residing    matched        walking  \n",
       "1        privately  inadvertently  miserably    comfortably  \n",
       "2             moon        panther      guard  country-dance  \n",
       "3         doubling           paid     naming       carrying  \n",
       "4           parcel           sign      sheet         chorus  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(questions) as instream:\n",
    "    csvreader = csv.reader(instream)\n",
    "    lines = list(csvreader)\n",
    "qs_df = pd.DataFrame(lines[1:], columns=lines[0])\n",
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id answer\n",
       "0  1      c\n",
       "1  2      a\n",
       "2  3      d\n",
       "3  4      c\n",
       "4  5      d"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(answers) as instream:\n",
    "    csvreader = csv.reader(instream)\n",
    "    lines = list(csvreader)\n",
    "ans_df = pd.DataFrame(lines[1:], columns=lines[0])\n",
    "ans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the n-gram language model\n",
    "\n",
    "This language model will predict the words from the questions with use of a **Unigram**, **Bigram**, and **Trigram**. \n",
    "\n",
    "The model was first developed in the *Advanced Natural Language Processing* module at the University of Sussex, then redeveloped by me to include a trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "3EQSHyuHGuwM"
   },
   "outputs": [],
   "source": [
    "def get_training_testing(training_dir=trainingdir, split=0.5):\n",
    "\n",
    "    filenames = os.listdir(training_dir)\n",
    "    n = len(filenames)\n",
    "    print(f\"There are {n} files in the training directory: {training_dir}\")\n",
    "    random.seed(64) \n",
    "    random.shuffle(filenames)\n",
    "    index = int(n*split)\n",
    "    trainingfiles=filenames[:index]\n",
    "    heldoutfiles=filenames[index:]\n",
    "    \n",
    "    return trainingfiles, heldoutfiles\n",
    "\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    \n",
    "    \"\"\" This LM can train and predict words using a unigram, bigram or trigram.\n",
    "    The n-grams are smoothed with Absolute Discounting, Kneser-Ney and a OOV threshold.\"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, known=2, discount=0.75, trainingdir=trainingdir, files=[]):\n",
    "        self.training_dir = trainingdir\n",
    "        self.files = files\n",
    "        self.discount = discount\n",
    "        self.known = known\n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self.trigram = {}\n",
    "        self.count_token = {}\n",
    "\n",
    "        self._processfiles()\n",
    "        self._make_unknowns()\n",
    "        self._kneser_ney()\n",
    "        self._convert_to_probs()\n",
    "      \n",
    "    \n",
    "    def _processline(self,line):\n",
    "        tokens = [\"__START\"] + word_tokenize(line) + [\"__END\"]\n",
    "        previous = \"__END\"\n",
    "        prev_tri = [\"__END\", \"__END\"]\n",
    "        for token in tokens:\n",
    "            # Unigram Frequency distribution (+ vocab_size)\n",
    "            self.unigram[token] = self.unigram.get(token,0) + 1\n",
    "\n",
    "            # Count tokens\n",
    "            self.count_token[token] = self.count_token.get(token,0) + 1\n",
    "              \n",
    "            # Bigram Frequency Distribution\n",
    "            current = self.bigram.get(previous,{})\n",
    "            current[token] = current.get(token,0) + 1\n",
    "            self.bigram[previous] = current\n",
    "\n",
    "            # Trigram Frequency Distribution\n",
    "            prev_tri[1], prev_tri[0] = prev_tri[0], prev_tri[1]\n",
    "            prev_tri[1] = previous\n",
    "            current = self.trigram.get(tuple(prev_tri), {})\n",
    "            current[token] = current.get(token, 0) + 1\n",
    "            self.trigram[tuple(prev_tri)] = current\n",
    "\n",
    "            previous = token\n",
    "    \n",
    "    def _processfiles(self):\n",
    "        \n",
    "        for afile in self.files:\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir, afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line) > 0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"UnicodeDecodeError processing {afile}: ignoring rest of file\")\n",
    "      \n",
    "              \n",
    "    def _convert_to_probs(self):\n",
    "          \n",
    "        self.unigram = {k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        self.bigram = {key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
    "        self.trigram = {key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.trigram.items()}\n",
    "        self.bi_kn = {k:v/sum(self.bi_kn.values()) for (k,v) in self.bi_kn.items()}\n",
    "        self.tri_kn = {k:v/sum(self.tri_kn.values()) for (k,v) in self.tri_kn.items()}\n",
    "        \n",
    "        \n",
    "    def get_prob(self, token, context=\"\", methodparams={}):\n",
    "        if methodparams.get(\"method\", \"unigram\") == \"unigram\":\n",
    "            return self.unigram.get(token, self.unigram.get(\"__UNK\", 0))\n",
    "          \n",
    "        elif methodparams.get(\"method\", \"bigram\") == \"bigram\": \n",
    "            if methodparams.get(\"smoothing\", \"kneser-ney\") == \"kneser-ney\":\n",
    "                unidist = self.bi_kn\n",
    "            else:\n",
    "                unidist = self.unigram\n",
    "\n",
    "            bigram = self.bigram.get(context[-1], self.bigram.get(\"__UNK\",{}))\n",
    "            bi_p = bigram.get(token,bigram.get(\"__UNK\",0))\n",
    "            lmbda = bigram[\"__DISCOUNT\"]\n",
    "            uni_p = unidist.get(token,unidist.get(\"__UNK\",0))\n",
    "            p = bi_p + lmbda * uni_p            \n",
    "            return p\n",
    "\n",
    "        elif methodparams.get(\"method\", \"trigram\") == \"trigram\":\n",
    "            if methodparams.get(\"smoothing\", \"kneser-ney\") == \"kneser-ney\":\n",
    "                unidist = self.tri_kn\n",
    "                unidist_bi = self.bi_kn\n",
    "            else:\n",
    "                unidist_bi = self.unigram\n",
    "\n",
    "            if len(context) < 2:\n",
    "                context = [\"__END\", context[0]]\n",
    "            trigram = self.trigram.get(tuple(context[-2:]), self.trigram.get(\"__UNK\", {}))\n",
    "            tri_p = trigram.get(token, trigram.get(\"__UNK\", 0))\n",
    "            lmbda_tri = trigram[\"__DISCOUNT\"]\n",
    "\n",
    "            bigram = self.bigram.get(context[-1], self.bigram.get(\"__UNK\", {}))\n",
    "            bi_p = bigram.get(token, bigram.get(\"__UNK\", 0))\n",
    "            lmbda_bi = bigram[\"__DISCOUNT\"]\n",
    "            uni_p = unidist_bi.get(token, unidist_bi.get(\"__UNK\", 0))\n",
    "\n",
    "            p = tri_p + (lmbda_tri * bi_p) + (lmbda_bi * uni_p)      \n",
    "            return p\n",
    "      \n",
    "    def compute_prob_line(self, line, methodparams={}):\n",
    "\n",
    "        \"\"\"Computes the probability of each line of the document\"\"\"\n",
    "        \n",
    "        tokens = [\"__START\"] + word_tokenize(line) + [\"__END\"]\n",
    "        acc = 0\n",
    "        for i, token in enumerate(tokens[1:]):\n",
    "            acc += math.log(self.get_prob(token, tokens[:i+1], methodparams))\n",
    "        return acc, len(tokens[1:])\n",
    "      \n",
    "    def compute_probability(self, filenames=[], methodparams={}):\n",
    "        # computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames == []:\n",
    "            filenames = self.files\n",
    "      \n",
    "        total_p = 0\n",
    "        total_N = 0\n",
    "        for i, afile in enumerate(filenames):\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir, afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line) > 0:\n",
    "                            p,N = self.compute_prob_line(line, methodparams=methodparams)\n",
    "                            total_p += p\n",
    "                            total_N += N\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"UnicodeDecodeError processing file {afile}: ignoring rest of file\")\n",
    "        return total_p, total_N\n",
    "      \n",
    "    def compute_perplexity(self, filenames=[], methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"}):\n",
    "        \n",
    "        \"\"\"Perplexity is a measurement of how well a probability \n",
    "        distribution or probability model predicts a sample. A low \n",
    "        perplexity indicates the probability distribution is good \n",
    "        at predicting the sample\"\"\"\n",
    "        \n",
    "        if methodparams.get(\"method\") in ['unigram', 'bigram']:\n",
    "            p, N = self.compute_probability(filenames=filenames, methodparams=methodparams)\n",
    "            pp = math.exp(-p/N)\n",
    "    \n",
    "      \n",
    "    def _make_unknowns(self):\n",
    "        \n",
    "        \"\"\"Removing words below a threshold frequency and\n",
    "        moving their values into a unknown sequence (\"_UNK\").\"\"\"\n",
    "        \n",
    "        # Unigram\n",
    "        unknown = 0\n",
    "        self.number_unknowns = 0\n",
    "        for (k,v) in list(self.unigram.items()):\n",
    "            if v < self.known:\n",
    "                del self.unigram[k]\n",
    "                self.unigram[\"__UNK\"] = self.unigram.get(\"__UNK\",0) + v\n",
    "                self.number_unknowns += 1\n",
    "        # Bigram\n",
    "        for (k,adict) in list(self.bigram.items()):\n",
    "            for (kk,v) in list(adict.items()):\n",
    "                isknown = self.unigram.get(kk,0)\n",
    "                if isknown == 0 and not kk == \"__DISCOUNT\":\n",
    "                    adict[\"__UNK\"] = adict.get(\"__UNK\",0) + v\n",
    "                    del adict[kk]\n",
    "            isknown = self.unigram.get(k,0)\n",
    "            if isknown == 0:\n",
    "                del self.bigram[k]\n",
    "                current = self.bigram.get(\"__UNK\",{})\n",
    "                current.update(adict)\n",
    "                self.bigram[\"__UNK\"] = current\n",
    "            else:\n",
    "                self.bigram[k] = adict\n",
    "        # Trigram\n",
    "        for (k,adict) in list(self.trigram.items()):\n",
    "            for (kk,v) in list(adict.items()):\n",
    "                isknown = self.unigram.get(kk,0)\n",
    "                if isknown == 0 and not kk == \"__DISCOUNT\":\n",
    "                    adict[\"__UNK\"] = adict.get(\"__UNK\",0) + v\n",
    "                    del adict[kk]\n",
    "            prev_1, prev_2 = k\n",
    "            isknown_1, isknown_2 = self.unigram.get(prev_1,0), self.unigram.get(prev_2,0)\n",
    "            if isknown_1 == 0 or isknown_2 == 0:\n",
    "                del self.trigram[k]\n",
    "                current = self.trigram.get(\"__UNK\",{})\n",
    "                current.update(adict)\n",
    "                self.trigram[\"__UNK\"] = current\n",
    "            else:\n",
    "                self.trigram[k] = adict\n",
    "            \n",
    "                  \n",
    "    def _kneser_ney(self):\n",
    "\n",
    "\n",
    "        self.bigram = {k:{kk:value-self.discount for (kk,value) in adict.items()} for (k,adict) in self.bigram.items()}\n",
    "        self.trigram = {k:{kk:value-self.discount for (kk,value) in adict.items()} for (k,adict) in self.trigram.items()}\n",
    "          \n",
    "        # For each word, store the total amount of the discount so that the total is the same \n",
    "        # i.e., so we are reserving this as probability mass\n",
    "        for k in self.bigram.keys():\n",
    "            lamb = len(self.bigram[k])\n",
    "            self.bigram[k][\"__DISCOUNT\"] = lamb * self.discount\n",
    "                  \n",
    "        for k in self.trigram.keys():\n",
    "            lamb = len(self.trigram[k])\n",
    "            self.trigram[k][\"__DISCOUNT\"] = lamb * self.discount\n",
    "\n",
    "        # Work out kneser-ney unigram probabilities\n",
    "        # Count the number of contexts each word has been seen in\n",
    "        self.bi_kn = {}\n",
    "        for (k,adict) in self.bigram.items():\n",
    "            for kk in adict.keys():\n",
    "                self.bi_kn[kk] = self.bi_kn.get(kk,0) + 1\n",
    "\n",
    "        self.tri_kn = {}\n",
    "        for (k,adict) in self.trigram.items():\n",
    "            for kk in adict.keys():\n",
    "                self.tri_kn[kk] = self.tri_kn.get(kk,0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "fJYcugWhG-A6"
   },
   "outputs": [],
   "source": [
    "class question:\n",
    "    \n",
    "    def __init__(self, aline):\n",
    "        self.fields = aline\n",
    "    \n",
    "    def get_field(self, field):\n",
    "        return self.fields[question.colnames[field]]\n",
    "    \n",
    "    def add_answer(self, fields):\n",
    "        self.answer = fields[1]\n",
    "\n",
    "    def get_tokens(self):\n",
    "        return [\"__START\"] + word_tokenize(self.fields[question.colnames[\"question\"]]) + [\"__END\"]\n",
    "\n",
    "    def get_left_context(self, window=1, target=\"_____\"):\n",
    "        found = -1\n",
    "        sent_tokens = self.get_tokens()\n",
    "        for i, token in enumerate(sent_tokens):\n",
    "            if token == target:\n",
    "                found = i\n",
    "                break  \n",
    "        if found >- 1:\n",
    "            return sent_tokens[i-window:i]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def get_right_context(self, window=1, target=\"_____\"):\n",
    "        found = -1\n",
    "        sent_tokens = self.get_tokens()\n",
    "        for i,token in enumerate(sent_tokens):\n",
    "            if token == target:\n",
    "                found = i\n",
    "                break\n",
    "        if found >- 1:\n",
    "            return sent_tokens[found + 1:found + window + 1]  \n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def choose(self, lm, method=\"bigram\", smoothing=\"Kneser-ney\", choices=[]):\n",
    "        \n",
    "        \"\"\" Chooses the specific n-gram model to get \n",
    "        the predicted answer from the alternatives.\"\"\"\n",
    "        \n",
    "        if choices == []:\n",
    "            choices = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "\n",
    "        if method == \"bigram\":\n",
    "            rc = self.get_right_context(window=1)\n",
    "            lc = self.get_left_context(window=1)\n",
    "            probs = [lm.get_prob(rc[0],[self.get_field(ch+\")\")],\n",
    "                                 methodparams={\"method\":method.split(\"_\")[0], \"smoothing\":smoothing}) \n",
    "                    * lm.get_prob(self.get_field(ch+\")\"),lc,\n",
    "                                  methodparams={\"method\":method.split(\"_\")[0], \"smoothing\":smoothing}) for ch in choices]\n",
    "\n",
    "        elif method == \"trigram\":\n",
    "            rc = self.get_right_context(window=2)\n",
    "            lc = self.get_left_context(window=2)\n",
    "            probs = [lm.get_prob(self.get_field(ch+\")\"), lc, \n",
    "                                 methodparams={\"method\":method.split(\"_\")[0], \"smoothing\":smoothing})\n",
    "                    * lm.get_prob(rc[0], [lc[-1]] + [self.get_field(ch+\")\")],\n",
    "                                  methodparams={\"method\":method.split(\"_\")[0], \"smoothing\":smoothing})\n",
    "                    * lm.get_prob(rc[1], [self.get_field(ch+\")\")] + [rc[0]],\n",
    "                                  methodparams={\"method\":method.split(\"_\")[0], \"smoothing\":smoothing}) for ch in choices]\n",
    "        \n",
    "        else:\n",
    "            context = self.get_left_context(window=1)\n",
    "            probs = [lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n",
    "        \n",
    "        maxprob = max(probs)\n",
    "        bestchoices = [ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
    "\n",
    "        return np.random.choice(bestchoices), probs\n",
    "      \n",
    "    def predict(self,lm,method=\"bigram\", smoothing=\"kneser-ney\"):\n",
    "        \n",
    "        \n",
    "        return self.choose(lm, method=method, smoothing=smoothing, choices=[])\n",
    "\n",
    "    def predict_and_score(self,lm,method=\"bigram\", smoothing=\"kneser-ney\"):\n",
    "        \n",
    "        \"\"\"Runs the n-gram LM and returns:\n",
    "        1 if the predicted answer is correct\n",
    "        0 if the predicted answer is incorrect\"\"\"\n",
    "        \n",
    "        prediction, probs = self.predict(lm,method=method,smoothing=smoothing)\n",
    "\n",
    "        if prediction == self.answer:\n",
    "            return 1, prediction, probs\n",
    "        else:\n",
    "            return 0, prediction, probs\n",
    "\n",
    "class scc_reader:\n",
    "      \n",
    "    def __init__(self,qs,ans):\n",
    "        self.qs=qs\n",
    "        self.ans=ans\n",
    "        self.read_files()\n",
    "      \n",
    "    def read_files(self):\n",
    "        \n",
    "        #read in the question file\n",
    "        with open(self.qs) as instream:\n",
    "            csvreader = csv.reader(instream)\n",
    "            qlines = list(csvreader)\n",
    "      \n",
    "        #store the column names as a reverse index so they can be used to reference parts of the question\n",
    "        question.colnames = {item:i for i,item in enumerate(qlines[0])}\n",
    "        \n",
    "        #create a question instance for each line of the file (other than heading line)\n",
    "        self.questions = [question(qline) for qline in qlines[1:]]\n",
    "        \n",
    "        #read in the answer file\n",
    "        with open(self.ans) as instream:\n",
    "            csvreader = csv.reader(instream)\n",
    "            alines = list(csvreader)\n",
    "            \n",
    "        #add answers to questions so predictions can be checked    \n",
    "        for q,aline in zip(self.questions,alines[1:]):\n",
    "            q.add_answer(aline)\n",
    "        \n",
    "    def get_field(self,field):\n",
    "        return [q.get_field(field) for q in self.questions] \n",
    "    \n",
    "    def predict(self,method=\"bigram\"):\n",
    "        return [q.predict(method=method) for q in self.questions]\n",
    "    \n",
    "    def predict_and_score(self,lm,method=\"bigram\",smoothing=\"kneser-ney\"):\n",
    "        \"\"\"\n",
    "        Return the accuracy of the ngram, along with all its predictions and the \n",
    "        probability distribution of the options of each question\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        scores = []\n",
    "        total_probs = []\n",
    "        for q in self.questions:\n",
    "            score, pred, probs = q.predict_and_score(lm,method=method, smoothing=smoothing)\n",
    "            scores.append(score)\n",
    "            predictions.append(pred)\n",
    "            total_probs.append(probs)\n",
    "\n",
    "        return sum(scores)/len(scores), predictions, total_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwCXxUr_KDZ9"
   },
   "source": [
    "### Hyperparamater tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mOS4TPjG_f_",
    "outputId": "809df03c-54b4-4c62-8bb3-33ac33e66970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lm on 10 documents.\n",
      "Training lm on 10 documents.\n",
      "Training lm on 10 documents.\n",
      "Training lm on 10 documents.\n",
      "Training lm on 10 documents.\n",
      "Training lm on 10 documents.\n",
      "Training lm on 10 documents.\n",
      "Training lm on 10 documents.\n",
      "Training lm on 10 documents.\n",
      "Training lm on 50 documents.\n",
      "Training lm on 50 documents.\n",
      "Training lm on 50 documents.\n",
      "Training lm on 50 documents.\n",
      "Training lm on 50 documents.\n",
      "Training lm on 50 documents.\n",
      "Training lm on 50 documents.\n",
      "Training lm on 50 documents.\n",
      "Training lm on 50 documents.\n",
      "Training lm on 100 documents.\n",
      "Training lm on 100 documents.\n",
      "Training lm on 100 documents.\n",
      "Training lm on 100 documents.\n",
      "Training lm on 100 documents.\n",
      "Training lm on 100 documents.\n",
      "Training lm on 100 documents.\n",
      "Training lm on 100 documents.\n",
      "Training lm on 100 documents.\n"
     ]
    }
   ],
   "source": [
    "file_path = os.listdir(trainingdir)\n",
    "n_files = [10, 50, 100]\n",
    "threshold = [2, 5, 10]\n",
    "discount = [0.5, 0.75, 1.00]\n",
    "MAX_FILES = 2\n",
    "results = []\n",
    "predictions = []\n",
    "ngram_probs = []\n",
    "\n",
    "iter = 0\n",
    "for n in n_files:\n",
    "    for i in threshold:\n",
    "        for d in discount:\n",
    "            print(f\"Training lm on {n} documents.\")\n",
    "            mylm = language_model(known=i, discount=d, files=file_path[:n])\n",
    "            SCC = scc_reader(questions, answers)\n",
    "            unigram_acc, unigram_pred, unigram_probs = SCC.predict_and_score(mylm, method=\"unigram\", smoothing='kneser-ney')\n",
    "            bigram_acc, bigram_pred, bigram_probs = SCC.predict_and_score(mylm, method=\"bigram\", smoothing='kneser-ney')\n",
    "            trigram_acc, trigram_pred, trigram_probs = SCC.predict_and_score(mylm, method=\"trigram\", smoothing='kneser-ney')\n",
    "\n",
    "            results.append((n, i, d, round(unigram_acc * 100, 3), round(bigram_acc * 100, 3), round(trigram_acc * 100, 3)))\n",
    "            predictions.append((unigram_pred, bigram_pred, trigram_pred))\n",
    "            ngram_probs.append((unigram_probs, bigram_probs, trigram_probs))\n",
    "            iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1\n",
    "The table displays how the accuracy is affected by changes in three hyper-parameters:\n",
    " - The number of files used to train the model on (n_files)\n",
    " - The lower threshold of words to be removed from the vocabulary (OOV_Threshold)\n",
    " - A discount to remove from the frequency of every word in the vocabulary (Discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "LUlAv8JNHE0z",
    "outputId": "370f6e5b-0de7-4544-f410-981b9cb64f7e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_files</th>\n",
       "      <th>OOV_Threshold</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Unigram_Acc</th>\n",
       "      <th>Bigram_Acc</th>\n",
       "      <th>Trigram_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>20.865</td>\n",
       "      <td>19.808</td>\n",
       "      <td>21.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>20.962</td>\n",
       "      <td>19.327</td>\n",
       "      <td>20.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>21.154</td>\n",
       "      <td>19.423</td>\n",
       "      <td>20.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>17.308</td>\n",
       "      <td>18.077</td>\n",
       "      <td>22.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>15.865</td>\n",
       "      <td>18.558</td>\n",
       "      <td>21.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>16.635</td>\n",
       "      <td>19.135</td>\n",
       "      <td>19.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>17.404</td>\n",
       "      <td>21.058</td>\n",
       "      <td>21.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.75</td>\n",
       "      <td>15.769</td>\n",
       "      <td>19.904</td>\n",
       "      <td>21.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>14.423</td>\n",
       "      <td>18.750</td>\n",
       "      <td>17.308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>24.135</td>\n",
       "      <td>20.000</td>\n",
       "      <td>25.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>24.231</td>\n",
       "      <td>20.385</td>\n",
       "      <td>24.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>24.135</td>\n",
       "      <td>21.346</td>\n",
       "      <td>23.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>23.462</td>\n",
       "      <td>18.750</td>\n",
       "      <td>21.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>23.365</td>\n",
       "      <td>18.846</td>\n",
       "      <td>21.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>23.269</td>\n",
       "      <td>19.904</td>\n",
       "      <td>20.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>22.596</td>\n",
       "      <td>19.327</td>\n",
       "      <td>20.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>0.75</td>\n",
       "      <td>22.404</td>\n",
       "      <td>19.712</td>\n",
       "      <td>20.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>22.885</td>\n",
       "      <td>20.096</td>\n",
       "      <td>19.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>24.135</td>\n",
       "      <td>22.404</td>\n",
       "      <td>27.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>24.038</td>\n",
       "      <td>22.212</td>\n",
       "      <td>27.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>24.038</td>\n",
       "      <td>22.981</td>\n",
       "      <td>27.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>24.135</td>\n",
       "      <td>21.827</td>\n",
       "      <td>25.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>24.038</td>\n",
       "      <td>21.923</td>\n",
       "      <td>24.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>24.135</td>\n",
       "      <td>21.442</td>\n",
       "      <td>23.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>22.788</td>\n",
       "      <td>20.192</td>\n",
       "      <td>23.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.75</td>\n",
       "      <td>23.365</td>\n",
       "      <td>20.481</td>\n",
       "      <td>22.308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>23.269</td>\n",
       "      <td>20.962</td>\n",
       "      <td>22.019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_files  OOV_Threshold  Discount  Unigram_Acc  Bigram_Acc  Trigram_Acc\n",
       "0        10              2      0.50       20.865      19.808       21.442\n",
       "1        10              2      0.75       20.962      19.327       20.673\n",
       "2        10              2      1.00       21.154      19.423       20.192\n",
       "3        10              5      0.50       17.308      18.077       22.212\n",
       "4        10              5      0.75       15.865      18.558       21.731\n",
       "5        10              5      1.00       16.635      19.135       19.712\n",
       "6        10             10      0.50       17.404      21.058       21.923\n",
       "7        10             10      0.75       15.769      19.904       21.923\n",
       "8        10             10      1.00       14.423      18.750       17.308\n",
       "9        50              2      0.50       24.135      20.000       25.865\n",
       "10       50              2      0.75       24.231      20.385       24.615\n",
       "11       50              2      1.00       24.135      21.346       23.462\n",
       "12       50              5      0.50       23.462      18.750       21.731\n",
       "13       50              5      0.75       23.365      18.846       21.154\n",
       "14       50              5      1.00       23.269      19.904       20.288\n",
       "15       50             10      0.50       22.596      19.327       20.192\n",
       "16       50             10      0.75       22.404      19.712       20.096\n",
       "17       50             10      1.00       22.885      20.096       19.231\n",
       "18      100              2      0.50       24.135      22.404       27.404\n",
       "19      100              2      0.75       24.038      22.212       27.692\n",
       "20      100              2      1.00       24.038      22.981       27.115\n",
       "21      100              5      0.50       24.135      21.827       25.769\n",
       "22      100              5      0.75       24.038      21.923       24.615\n",
       "23      100              5      1.00       24.135      21.442       23.077\n",
       "24      100             10      0.50       22.788      20.192       23.462\n",
       "25      100             10      0.75       23.365      20.481       22.308\n",
       "26      100             10      1.00       23.269      20.962       22.019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(results, columns=['n_files', 'OOV_Threshold', 'Discount', 'Unigram_Acc', 'Bigram_Acc', 'Trigram_Acc'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameters for each model, let's see what how high the accuracy of each n-gram is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylm = language_model(known=2, discount=0.75, files=file_path)\n",
    "unigram_acc, unigram_pred, unigram_probs = SCC.predict_and_score(mylm, method=\"unigram\", smoothing='kneser-ney')\n",
    "bigram_acc, bigram_pred, bigram_probs = SCC.predict_and_score(mylm, method=\"bigram\", smoothing='kneser-ney')\n",
    "trigram_acc, trigram_pred, trigram_probs = SCC.predict_and_score(mylm, method=\"trigram\", smoothing='kneser-ney')\n",
    "\n",
    "res = [round(unigram_acc * 100, 3), round(bigram_acc * 100, 3), round(trigram_acc * 100, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2\n",
    "The table shows each n-gram in the language model with their optimized hyper-parameter settings from table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joakim Torsvik\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Performance')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(res, index=[\"Unigram\", \"Bigram\", \"Trigram\"]).transpose()\n",
    "df.head()\n",
    "\n",
    "sns.barplot([\"Unigram\", \"Bigram\", \"Trigram\"], res)\n",
    "plt.title(\"Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions where 3/3 models were correct: 68\n",
      "Number of questions where 2/3 models were correct: 184\n",
      "Number of questions where 1/3 models were correct: 286\n",
      "Number of questions where 0/3 models were correct: 502\n"
     ]
    }
   ],
   "source": [
    "uni = []\n",
    "bi = []\n",
    "tri = []\n",
    "for i in range(len(unigram_pred)):\n",
    "    # Unigram\n",
    "    if ans_df['answer'][i] == unigram_pred[i]:\n",
    "        uni.append(1)\n",
    "    else:\n",
    "        uni.append(0) \n",
    "    # Bigram\n",
    "    if ans_df['answer'][i] == bigram_pred[i]:\n",
    "        bi.append(1)\n",
    "    else:\n",
    "        bi.append(0)\n",
    "    # Trigram\n",
    "    if ans_df['answer'][i] == trigram_pred[i]:\n",
    "        tri.append(1)\n",
    "    else:\n",
    "        tri.append(0)\n",
    "\n",
    "comparison = [(uni[i] + bi[i] + tri[i])/3 for i in range(len(uni))]\n",
    "print(f\"Number of questions where 3/3 models were correct: {sum([int(i) for i in comparison if i == 1])}\")\n",
    "print(f\"Number of questions where 2/3 models were correct: {sum([1 for i in comparison if round(i,3) == 0.667])}\")\n",
    "print(f\"Number of questions where 1/3 models were correct: {sum([1 for i in comparison if round(i,3) == 0.333])}\")\n",
    "print(f\"Number of questions where 0/3 models were correct: {sum([1 for i in comparison if round(i,3) == 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "570 questions were predicted similarly by all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If 1, all are correct.\n",
    "# If 0.667, 2 / 3 are correct\n",
    "# If 0.333, 1 / 3 is correct\n",
    "# If 0, all models are wrong\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(len(uni)), sorted(comparison, reverse=True))\n",
    "plt.title(\"Comparison of correct answers\")\n",
    "plt.xlabel(\"Questions\")\n",
    "plt.ylabel(\"Correct / Incorrect\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Entropy and Perplexity of the n-grams\n",
    "\n",
    "Entropy is defined as: $H(x) = - \\sum{P(x_i) \\times \\log{P(x_i)}} $\n",
    "\n",
    "Perplexity is defined as $H({\\tilde  {p}},q)=-\\sum _{x}{\\tilde  {p}}(x)\\log _{2}q(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probs, preds):\n",
    "    ix = []\n",
    "    for p in preds:\n",
    "        if p == 'a':\n",
    "            ix.append(0)\n",
    "        elif p == 'b':\n",
    "            ix.append(1)\n",
    "        elif p == 'c':\n",
    "            ix.append(2)\n",
    "        elif p == 'd':\n",
    "            ix.append(3)\n",
    "        elif p == 'e':\n",
    "            ix.append(4)\n",
    "    \n",
    "    return round(-sum([prob[ix[i]] * np.log2(prob[ix[i]]) for i, prob in enumerate(probs)]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unigram</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Trigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Entropy</th>\n",
       "      <td>1.035</td>\n",
       "      <td>1.815</td>\n",
       "      <td>0.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>731.507</td>\n",
       "      <td>139.927</td>\n",
       "      <td>47.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>24.904</td>\n",
       "      <td>25.288</td>\n",
       "      <td>32.308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unigram   Bigram  Trigram\n",
       "Entropy       1.035    1.815    0.967\n",
       "Perplexity  731.507  139.927   47.244\n",
       "Accuracy     24.904   25.288   32.308"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[entropy(unigram_probs, unigram_pred), \n",
    "                    entropy(bigram_probs, bigram_pred), \n",
    "                    entropy(trigram_probs, trigram_pred)],\n",
    "                   [round(mylm.compute_perplexity(methodparams = {'method':'unigram', 'smoothing': 'kneser-ney'}), 3),\n",
    "                    round(mylm.compute_perplexity(methodparams = {'method':'bigram', 'smoothing': 'kneser-ney'}), 3),\n",
    "                    round(mylm.compute_perplexity(methodparams = {'method':'trigram', 'smoothing': 'kneser-ney'}), 3)],res],\n",
    "                  columns=['Unigram', 'Bigram', 'Trigram'], index=['Entropy', 'Perplexity', 'Accuracy'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ANLP - Ngram.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
