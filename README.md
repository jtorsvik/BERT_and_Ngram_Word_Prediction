# BERT-and-N-gram-Word-Prediction
Using different N-gram sizes and BERT models to predict a key-word from sentences and analysing the context understanding of each model.

The data that has been used in this project comes from the Microsoft Research Sentence Completion Challenge and uses extracted sentences from Sherlock Holmes novels, written by Sir Arthur Conan, in the Gutenberg corpus. From each sentence, a key word has been removed and the models then needs to predict the correct word out of 5 possible alternatives. After this, the model has been analysed see which model has most correct predictions and where the models fails to predict correctly.

The models used to complete the challenge are:
N-gram models - Unigram and Bigram models first developed by Dr. Julie Weeds of the University of Sussex, then extended to include Trigrams by the author.
Pre-trained BERT-models and RoBERTa-models both with and without case-sensitivity.

The attached PDF-file is the submitted report from this project, handed in on April 2022, and includes a detailed description of the problem and the models, as well as the results of the analyses made.
